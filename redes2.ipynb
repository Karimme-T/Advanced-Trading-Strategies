{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9478e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85380d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_notebook_y_extraer_vars(nb_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Ejecuta secuencialmente las celdas de código del notebook dado y devuelve\n",
    "    un diccionario con las variables definidas (namespace).\n",
    "\n",
    "    No modifica el .ipynb original.\n",
    "    \"\"\"\n",
    "    import nbformat, runpy\n",
    "\n",
    "    nb = nbformat.read(str(nb_path), as_version=4)\n",
    "    # Ambiente aislado donde se ejecutarán las celdas\n",
    "    ns = {}\n",
    "    # Ejecutar solo celdas de código en orden\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == \"code\":\n",
    "            code = cell.source\n",
    "            if code.strip():\n",
    "                exec(compile(code, filename=\"<notebook_cell>\", mode=\"exec\"), ns, ns)\n",
    "    return ns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac1afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_Xy_desde_namespace(ns: dict):\n",
    "    \"\"\"\n",
    "    Intenta recuperar los DataFrames preprocesados del notebook:\n",
    "      - train_scaled (features estandarizadas por día, con columna 'date' y quizás 'Close', etc.)\n",
    "      - train_df     (debe contener columna 'signal' con {-1,0,1})\n",
    "    Construye X (features) e y (etiquetas) alineados por índice temporal.\n",
    "    \"\"\"\n",
    "    if \"train_scaled\" not in ns:\n",
    "        raise RuntimeError(\"No se encontró 'train_scaled' en el notebook. Asegúrate de ejecutar feature_eng.ipynb antes.\")\n",
    "    if \"train_df\" not in ns:\n",
    "        raise RuntimeError(\"No se encontró 'train_df' en el notebook. Asegúrate de ejecutar feature_eng.ipynb antes.\")\n",
    "\n",
    "    train_scaled = ns[\"train_scaled\"].copy()\n",
    "    train_df = ns[\"train_df\"].copy()\n",
    "\n",
    "    # Alinear por índice (suponemos orden temporal ya preservado en el notebook)\n",
    "    df_merged = train_scaled.join(train_df[[\"signal\"]], how=\"inner\")\n",
    "    # Features = todo salvo 'date' y 'signal'\n",
    "    X = df_merged.drop(columns=[c for c in [\"date\", \"signal\"] if c in df_merged.columns], errors=\"ignore\").values.astype(np.float32)\n",
    "    y_signal = df_merged[\"signal\"].astype(int).values  # {-1,0,1}\n",
    "\n",
    "    # Mapear a 0..K-1 para Keras\n",
    "    clases_unicas = sorted(np.unique(y_signal).tolist())\n",
    "    clase2idx = {c: i for i, c in enumerate(clases_unicas)}\n",
    "    idx2clase = {i: c for c, i in clase2idx.items()}\n",
    "    y = np.array([clase2idx[c] for c in y_signal], dtype=np.int64)\n",
    "\n",
    "    return SimpleNamespace(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        clases_unicas=clases_unicas,\n",
    "        clase2idx=clase2idx,\n",
    "        idx2clase=idx2clase,\n",
    "        train_scaled=train_scaled,\n",
    "        train_df=train_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107e7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_temporal(X, y, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Split temporal: primeros registros -> train, luego val y test.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    n_test = int(n * test_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "    n_train = n - n_val - n_test\n",
    "    return (X[:n_train], y[:n_train],\n",
    "            X[n_train:n_train+n_val], y[n_train:n_train+n_val],\n",
    "            X[n_train+n_val:], y[n_train+n_val:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1c9925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_class_weights(y_train):\n",
    "    clases = np.unique(y_train)\n",
    "    pesos = compute_class_weight(class_weight=\"balanced\", classes=clases, y=y_train)\n",
    "    return {int(c): float(w) for c, w in zip(clases, pesos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a64f5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_mlp(n_features, n_clases, hidden=(128, 64), dropout=0.2, lr=1e-3):\n",
    "    inputs = keras.Input(shape=(n_features,))\n",
    "    x = inputs\n",
    "    for h in hidden:\n",
    "        x = layers.Dense(h, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dense(n_clases, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"MLP_Baseline\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c6dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_ventanas(X, y, window=32, horizon=1, step=1):\n",
    "    \"\"\"\n",
    "    Convierte series diarias X (N, F) en secuencias para CNN: (M, window, F)\n",
    "    La etiqueta es la del tiempo t + horizon en el extremo de la ventana.\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    fin = N - window - horizon + 1\n",
    "    if fin <= 0:\n",
    "        raise ValueError(\"Datos insuficientes para la ventana/horizonte.\")\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(0, fin, step):\n",
    "        X_seq.append(X[i:i+window])\n",
    "        y_seq.append(y[i+window-1 + horizon])\n",
    "    return np.asarray(X_seq, dtype=np.float32), np.asarray(y_seq, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29024c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_cnn_1d(window, n_features, n_clases,\n",
    "                     filtros=(64, 128), kernel_size=3, pool_size=2, dropout=0.3, lr=1e-3):\n",
    "    inputs = keras.Input(shape=(window, n_features))\n",
    "    x = inputs\n",
    "    for f in filtros:\n",
    "        x = layers.Conv1D(filters=f, kernel_size=kernel_size, padding=\"causal\", activation=\"relu\")(x)\n",
    "        x = layers.MaxPooling1D(pool_size=pool_size)(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(n_clases, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"CNN1D_Temporal\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c993919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar(model, X_test, y_test, idx2clase):\n",
    "    proba = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.argmax(proba, axis=1)\n",
    "    target_names = [str(idx2clase[i]) for i in sorted(idx2clase.keys())]\n",
    "    print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nReporte de clasificación:\\n\", classification_report(y_test, y_pred, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7799af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_mlp(ns_data, epochs=200, batch_size=256):\n",
    "    X, y = ns_data.X, ns_data.y\n",
    "    Xtr, ytr, Xva, yva, Xte, yte = split_temporal(X, y, val_ratio=0.15, test_ratio=0.15)\n",
    "    cw = preparar_class_weights(ytr)\n",
    "    model = construir_mlp(n_features=X.shape[1], n_clases=len(np.unique(y)))\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_loss\"),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-5),\n",
    "    ]\n",
    "    model.fit(Xtr, ytr, validation_data=(Xva, yva),\n",
    "              epochs=epochs, batch_size=batch_size, class_weight=cw, callbacks=callbacks, verbose=1)\n",
    "    evaluar(model, Xte, yte, ns_data.idx2clase)\n",
    "    model.save(\"mlp_baseline.keras\")\n",
    "    print(\"✔ Guardado: mlp_baseline.keras\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a837883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_cnn(ns_data, window=32, horizon=1, epochs=200, batch_size=256):\n",
    "    X, y = ns_data.X, ns_data.y\n",
    "    # Ventanas sobre TODO el histórico en escala ya estandarizada\n",
    "    X_seq, y_seq = construir_ventanas(X, y, window=window, horizon=horizon, step=1)\n",
    "    n = len(X_seq)\n",
    "    n_test = int(n * 0.15)\n",
    "    n_val = int(n * 0.15)\n",
    "    n_train = n - n_val - n_test\n",
    "    Xtr, ytr = X_seq[:n_train], y_seq[:n_train]\n",
    "    Xva, yva = X_seq[n_train:n_train+n_val], y_seq[n_train:n_train+n_val]\n",
    "    Xte, yte = X_seq[n_train+n_val:], y_seq[n_train+n_val:]\n",
    "\n",
    "    cw = preparar_class_weights(ytr)\n",
    "    model = construir_cnn_1d(window=window, n_features=X.shape[1], n_clases=len(np.unique(y)))\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=12, restore_best_weights=True, monitor=\"val_loss\"),\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=6, factor=0.5, min_lr=1e-5),\n",
    "    ]\n",
    "    model.fit(Xtr, ytr, validation_data=(Xva, yva),\n",
    "              epochs=epochs, batch_size=batch_size, class_weight=cw, callbacks=callbacks, verbose=1)\n",
    "    evaluar(model, Xte, yte, ns_data.idx2clase)\n",
    "    model.save(\"cnn_temporal.keras\")\n",
    "    print(\"✔ Guardado: cnn_temporal.keras\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac91a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Continuación DL: MLP y CNN a partir de feature_eng.ipynb (sin modificarlo)\")\n",
    "    parser.add_argument(\"--nb\", required=True, help=\"Ruta al feature_eng.ipynb\")\n",
    "    parser.add_argument(\"--modelo\", choices=[\"mlp\", \"cnn\"], required=True, help=\"Modelo a entrenar\")\n",
    "    parser.add_argument(\"--window\", type=int, default=32, help=\"(CNN) Tamaño de ventana\")\n",
    "    parser.add_argument(\"--horizon\", type=int, default=1, help=\"(CNN) Horizonte de predicción\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=200, help=\"Épocas de entrenamiento\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=256, help=\"Tamaño de lote\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    nb_path = Path(args.nb)\n",
    "    if not nb_path.exists():\n",
    "        print(f\"[ERROR] No se encontró el notebook en: {nb_path}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"Ejecutando notebook para importar resultados → {nb_path}\")\n",
    "    ns = ejecutar_notebook_y_extraer_vars(nb_path)\n",
    "    print(\"✔ Notebook ejecutado. Extrayendo matrices X,y…\")\n",
    "    ns_data = extraer_Xy_desde_namespace(ns)\n",
    "\n",
    "    if args.modelo == \"mlp\":\n",
    "        entrenar_mlp(ns_data, epochs=args.epochs, batch_size=args.batch_size)\n",
    "    else:\n",
    "        entrenar_cnn(ns_data, window=args.window, horizon=args.horizon,\n",
    "                     epochs=args.epochs, batch_size=args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a66f47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NB_PATH = Path(\"feature_eng.ipynb\")\n",
    "WINDOW = 32\n",
    "HORIZON = 1\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b5cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo: X,y extraídos. Clases: [-1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "ns = ejecutar_notebook_y_extraer_vars(NB_PATH)\n",
    "data = extraer_Xy_desde_namespace(ns)\n",
    "print(\"Listo: X,y extraídos. Clases:\", data.clases_unicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac0fbdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.3927 - loss: 1.3369 - val_accuracy: 0.3899 - val_loss: 1.1600 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5022 - loss: 0.8844 - val_accuracy: 0.4137 - val_loss: 1.1179 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.5455 - loss: 0.8065 - val_accuracy: 0.4137 - val_loss: 1.0746 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.5818 - loss: 0.7209 - val_accuracy: 0.4524 - val_loss: 1.0461 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5831 - loss: 0.7221 - val_accuracy: 0.4643 - val_loss: 1.0035 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6194 - loss: 0.6351 - val_accuracy: 0.5357 - val_loss: 0.9599 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6162 - loss: 0.6488 - val_accuracy: 0.5565 - val_loss: 0.9319 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6442 - loss: 0.5927 - val_accuracy: 0.5863 - val_loss: 0.9166 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6671 - loss: 0.6251 - val_accuracy: 0.6101 - val_loss: 0.8909 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.6741 - loss: 0.5620 - val_accuracy: 0.6250 - val_loss: 0.8728 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6926 - loss: 0.5713 - val_accuracy: 0.6071 - val_loss: 0.8572 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 0.6843 - loss: 0.5445 - val_accuracy: 0.6131 - val_loss: 0.8421 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.6926 - loss: 0.5167 - val_accuracy: 0.6220 - val_loss: 0.8299 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7066 - loss: 0.4920 - val_accuracy: 0.6310 - val_loss: 0.8258 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.7167 - loss: 0.5034 - val_accuracy: 0.6220 - val_loss: 0.8213 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.7301 - loss: 0.4880 - val_accuracy: 0.6369 - val_loss: 0.8396 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7320 - loss: 0.4966 - val_accuracy: 0.6250 - val_loss: 0.8681 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7416 - loss: 0.4692 - val_accuracy: 0.5804 - val_loss: 0.9010 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7403 - loss: 0.4726 - val_accuracy: 0.5774 - val_loss: 0.8944 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7327 - loss: 0.4616 - val_accuracy: 0.6101 - val_loss: 0.8589 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7454 - loss: 0.4602 - val_accuracy: 0.6339 - val_loss: 0.8394 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7505 - loss: 0.4335 - val_accuracy: 0.6429 - val_loss: 0.8349 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.7581 - loss: 0.4435 - val_accuracy: 0.6250 - val_loss: 0.8708 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7683 - loss: 0.4082 - val_accuracy: 0.6250 - val_loss: 0.8822 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7626 - loss: 0.4168 - val_accuracy: 0.6280 - val_loss: 0.8857 - learning_rate: 5.0000e-04\n",
      "\n",
      "Matriz de confusión:\n",
      " [[ 64  31  18]\n",
      " [  9  26  52]\n",
      " [  4  12 120]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.8312    0.5664    0.6737       113\n",
      "           0     0.3768    0.2989    0.3333        87\n",
      "           1     0.6316    0.8824    0.7362       136\n",
      "\n",
      "    accuracy                         0.6250       336\n",
      "   macro avg     0.6132    0.5825    0.5811       336\n",
      "weighted avg     0.6327    0.6250    0.6109       336\n",
      "\n",
      "✔ Guardado: mlp_baseline.keras\n"
     ]
    }
   ],
   "source": [
    "mlp = entrenar_mlp(data, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4edc6c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 332ms/step - accuracy: 0.4687 - loss: 1.5909 - val_accuracy: 0.3444 - val_loss: 1.3014 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step - accuracy: 0.4526 - loss: 1.4586 - val_accuracy: 0.2689 - val_loss: 1.2250 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - accuracy: 0.4790 - loss: 1.3557 - val_accuracy: 0.3142 - val_loss: 1.1673 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.5358 - loss: 1.1804 - val_accuracy: 0.3686 - val_loss: 1.1118 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.5636 - loss: 1.2326 - val_accuracy: 0.2810 - val_loss: 1.1718 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 146ms/step - accuracy: 0.5371 - loss: 1.1549 - val_accuracy: 0.2387 - val_loss: 1.2127 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step - accuracy: 0.5165 - loss: 1.1234 - val_accuracy: 0.2417 - val_loss: 1.2260 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy: 0.5313 - loss: 1.1406 - val_accuracy: 0.2417 - val_loss: 1.3459 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step - accuracy: 0.5468 - loss: 1.0777 - val_accuracy: 0.2779 - val_loss: 1.2673 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 188ms/step - accuracy: 0.5913 - loss: 1.0356 - val_accuracy: 0.2508 - val_loss: 1.2642 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - accuracy: 0.6094 - loss: 1.0423 - val_accuracy: 0.2387 - val_loss: 1.2949 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 160ms/step - accuracy: 0.5849 - loss: 1.0826 - val_accuracy: 0.2538 - val_loss: 1.2834 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.6023 - loss: 0.9954 - val_accuracy: 0.2659 - val_loss: 1.2562 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.5855 - loss: 0.9831 - val_accuracy: 0.2598 - val_loss: 1.2621 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 0.5759 - loss: 1.0108 - val_accuracy: 0.2900 - val_loss: 1.3227 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.5759 - loss: 0.9402 - val_accuracy: 0.2900 - val_loss: 1.2818 - learning_rate: 5.0000e-04\n",
      "\n",
      "Matriz de confusión:\n",
      " [[23 50 40]\n",
      " [19 32 35]\n",
      " [17 57 58]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.3898    0.2035    0.2674       113\n",
      "           0     0.2302    0.3721    0.2844        86\n",
      "           1     0.4361    0.4394    0.4377       132\n",
      "\n",
      "    accuracy                         0.3414       331\n",
      "   macro avg     0.3520    0.3383    0.3299       331\n",
      "weighted avg     0.3668    0.3414    0.3398       331\n",
      "\n",
      "✔ Guardado: cnn_temporal.keras\n"
     ]
    }
   ],
   "source": [
    "cnn = entrenar_cnn(data, window=WINDOW, horizon=HORIZON, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1aabe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.8.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
